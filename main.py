import streamlit as st
import torch
import torch.nn as nn
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences

st.set_page_config(page_title='AI vs Human', page_icon='ai.png', layout="centered", initial_sidebar_state="auto",
                   menu_items=None)

# Custom CSS for Comic Sans MS font
st.markdown(
    """
    <style>
    body, h1, h2, h3, h4, h5, h6, p, div, textarea, button {
        font-family: 'Comic Sans MS', sans-serif;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

st.markdown(
    "<center><h1 style='font-weight: 300; font-size: 32px; margin-bottom: 10px'>AI vs Human Text Classification</h1></center>",
    unsafe_allow_html=True,
)

# Model parameters
MAX_NB_WORDS = 50000
MAX_SEQUENCE_LENGTH = 250
EMBEDDING_DIM = 100

# Define the ImprovedGRUClassifier class
class ImprovedGRUClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, pretrained_embeddings=None):
        super(ImprovedGRUClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        if pretrained_embeddings is not None:
            self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))
            self.embedding.weight.requires_grad = False

        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, dropout=0.2, batch_first=True)
        self.drop = nn.Dropout(0.5)
        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc2 = nn.Linear(hidden_dim // 2, 1)

    def forward(self, x):
        x = self.embedding(x)
        x, hidden = self.gru(x)
        x = self.drop(x[:, -1, :])
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Load the tokenizer
with open('tokenizer.pickle', 'rb') as handle:
    tokenizer = pickle.load(handle)

# Load the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ImprovedGRUClassifier(MAX_NB_WORDS, EMBEDDING_DIM, 100)

# Load model state
model_state_path = 'ai_human.pth'
if torch.cuda.is_available():
    model.load_state_dict(torch.load(model_state_path))
else:
    model.load_state_dict(torch.load(model_state_path, map_location=torch.device('cpu')))

model.to(device)
model.eval()

# Function to preprocess the input text
def preprocess_data(new_data):
    new_sequences = tokenizer.texts_to_sequences(new_data)
    new_padded_sequences = pad_sequences(new_sequences, maxlen=MAX_SEQUENCE_LENGTH)
    return torch.tensor(new_padded_sequences, dtype=torch.long).to(device)

# Streamlit app
input_text = st.text_area("Enter the email text to classify", height=200)

if st.button("Classify"):
    if input_text:
        preprocessed_text = preprocess_data([input_text])
        with torch.no_grad():
            outputs = model(preprocessed_text).squeeze(1)
            prediction = torch.round(torch.sigmoid(outputs))

        if prediction == 1:
            st.error("This is generated by AI.")
        else:
            st.success("This is written by a Human.")
    else:
        st.warning("Please enter some text to classify.")
